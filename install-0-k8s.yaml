#更新、安装必要软件和docker
#--------------------------------------------------------------------
- hosts: all
  tasks:
  - name: 更新系统
    tags: update
    yum: name=* state=latest
    ignore_errors: yes
  - name: 安装必要软件
    tags: prepare,prepare-softs
    yum: name="{{ packages }}" state=present
    vars:
      packages:
      - nfs-utils
      - epel-release
      - net-tools
      - wget
      - bash-completion
      - lrzsz
      - unzip
      - yum-utils
      - device-mapper-persistent-data
      - lvm2
  - name: 检查防火墙服务是否存在
    tags: prepare,prepare-env
    shell: "if systemctl list-unit-files | grep firewalld > /dev/null; then echo 1; else echo 0; fi;"
    register: firewalld_exists
  - name: 关闭防火墙
    tags: prepare,prepare-env
    shell: "systemctl stop firewalld.service && systemctl disable firewalld.service"
    when: firewalld_exists.stdout == "1"
  - name: 禁用selinux
    tags: prepare,prepare-env
    shell: "setenforce 0 || 0"
    ignore_errors: yes
  - name: 禁用selinux
    tags: prepare,prepare-env
    shell: "sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config"
  - name: 禁用swap
    tags: prepare,prepare-env
    shell: swapoff -a
  - name: 开机启动禁用swap交换区
    tags: prepare,prepare-env
    shell: "sed -i 's/^.*swap*/#&/g' /etc/fstab"
  - name: 加载bridge、br_netfilter模块，并设置开机启动
    tags: prepare,prepare-env
    shell: "modprobe bridge && modprobe br_netfilter &&\ 
            touch /etc/modules-load.d/kubernetes.conf &&\ 
            echo 'bridge' > /etc/modules-load.d/kubernetes.conf &&\ 
            echo 'br_netfilter' >> /etc/modules-load.d/kubernetes.conf"
  - name: 设置iptables转发规则
    tags: prepare,prepare-env
    sysctl:
      name: "{{ item.name }}"
      value: "{{ item.value }}"
      sysctl_set: yes
      state: present
      reload: yes
    with_items:
    - {"name":"net.bridge.bridge-nf-call-iptables", "value":1}
    - {"name":"net.bridge.bridge-nf-call-ip6tables","value":1}
    - {"name":"vm.swappiness","value":0}
  - name: 判断是否配置过docker-ce.repo
    tags: prepare,prepare-docker
    shell: ls /etc/yum.repos.d | grep docker-ce.repo || echo 0
    register: check_docker_ce_repo
  - name: 配置docker-ce.repo（如果没配置过）
    tags: prepare,prepare-docker
    shell: "yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo"
    when: check_docker_ce_repo.stdout == "0"
  - name: 判断是否已安装docker
    tags: prepare,prepare-docker
    shell: "if systemctl list-unit-files | grep docker.service > /dev/null; then echo 1; else echo 0; fi;"
    register: check_docker
  - name: 安装containerd.io-1.2.6，因为docker-ce要求containerd版本大于1.2.2
    tags: prepare,prepare-docker
    yum: name=https://pubsrc.oss-cn-beijing.aliyuncs.com/containerd.io-1.2.6-3.3.el7.x86_64.rpm disable_gpg_check=yes state=present
    when: check_docker.stdout == "0"
  - name: 安装docker-ce-19.03.4
    tags: prepare,prepare-docker
    yum: name={{ packages }} state=present
    vars:
      packages:
      - docker-ce-19.03.4
      - docker-ce-cli-19.03.4
    when: check_docker.stdout == "0"
  - name: 配置daemon.json
    tags: prepare,prepare-docker
    copy: src=files/daemon.json dest=/etc/docker/
  - name: 启动 docker
    tags: prepare,prepare-docker
    shell: systemctl daemon-reload && systemctl enable docker && systemctl start docker
#安装k8s
#-------------------------------------------------------------------
- hosts: master,node
  tasks:
  - name: 配置k8s源
    tags: prepare,prepare-k8s
    copy: src=files/kubernetes.repo dest=/etc/yum.repos.d/
  - name: 判断是否已安装k8s
    tags: prepare,prepare-k8s
    shell: "if systemctl list-unit-files | grep kubelet.service > /dev/null; then echo 1; else echo 0; fi;"
    register: check_k8s
  - name: 安装k8s
    tags: prepare,prepare-k8s
    yum: name={{ packages }} state=present
    vars:
      packages:
      - kubelet-1.19.0-0
      - kubectl-1.19.0-0
      - kubeadm-1.19.0-0
      - kubernetes-cni-0.8.7-0
    when: check_k8s.stdout == "0"
  - name: 开机启动kubelet
    tags: prepare,prepare-k8s
    shell: systemctl daemon-reload && systemctl enable kubelet
- hosts: master[0]
  tasks:
  - name: 为ctrl节点安装kubectl
    tags: prepare,ctrl-kubectl
    fetch: src="/usr/bin/kubectl" dest="/usr/bin/kubectl" flat=yes
- hosts: ctrl
  tasks:
  - name: 修改kubectl权限
    tags: prepare,ctrl-kubectl
    file: path=/usr/bin/kubectl mode=0755
#配置master的hosts
#-------------------------------------------------------------------
- hosts: master
  tasks:
  - name: 配置master的host
    tags: loadbalance-master
    lineinfile:
      path: /etc/hosts
      line: "{{ groups.master[0] }} apiserver.{{ root_domain }}"
- hosts: node,ctrl
  tasks:
  - name: 配置node的host为内网负载均衡地址，apiserver负载均衡
    tags: loadbalance-nodes
    lineinfile:
      path: /etc/hosts
      line: "{{ loadbalance_inside_ip }} apiserver.{{ root_domain }}"
#生成集群所需的证书
#-------------------------------------------------------------------
- hosts: ctrl
  vars:
    etcds:
    - {"path":"/data/etcd/etcd0","name":"etcd0","host":"{{ groups.etcd[0] }}"}
    - {"path":"/data/etcd/etcd1","name":"etcd1","host":"{{ groups.etcd[1] }}"}
    - {"path":"/data/etcd/etcd2","name":"etcd2","host":"{{ groups.etcd[2] }}"}
  tasks:
  - name: 解决证书有效期为一年的问题
    tags: etcd,etcd-kubeadm
    shell: "rm -rf /usr/bin/kubeadm &&\ 
            docker pull {{ k8s_repo }}/kubeadm:1.19.0 &&\
            docker run --rm -v /tmp/kubeadm/:/tmp/kubeadm/ \
            {{ k8s_repo }}/kubeadm:1.19.0 sh -c 'cp /kubeadm /tmp/kubeadm/' &&\ 
            mv /tmp/kubeadm/kubeadm /usr/bin/ && chmod +x /usr/bin/kubeadm"
  - name: 先删除目录，防止多次安装的残留
    tags: etcd,etcd-dirs
    file:
      path: "{{ item }}"
      state: absent
    with_items:
    - /etc/kubernetes
    - /data/etcd
    - /var/lib/etcd
  - name: 创建etcd证书目录 /data/etcd/[nodeName]
    tags: etcd,etcd-dirs
    file:
      path: "{{ item.path }}"
      state: directory
    with_items: "{{ etcds }}"
  - name: 生成配置文件
    tags: etcd,etcd-yaml
    template:
      src: files/etcd.yaml
      dest: "{{ item.path }}/etcd.yaml"
    with_items: "{{ etcds }}"
  - name: 生成ca证书
    tags: etcd,etcd-ca
    shell: "kubeadm init phase certs etcd-ca"
  - name: 生成etcd其他证书
    tags: etcd,etcd-certs
    shell: "kubeadm init phase certs etcd-server --config={{ item.path }}/etcd.yaml &&\ 
            kubeadm init phase certs etcd-peer --config={{ item.path }}/etcd.yaml &&\ 
            kubeadm init phase certs etcd-healthcheck-client --config={{ item.path }}/etcd.yaml &&\ 
            kubeadm init phase certs apiserver-etcd-client --config={{ item.path }}/etcd.yaml &&\ 
            cp -R /etc/kubernetes/pki {{ item.path }} &&\ 
            find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete"
    with_items: "{{ etcds }}"
#安装etcd集群
#-------------------------------------------------------------------
- hosts: etcd
  tasks:
  - name: 先删除容器，防止多次安装的残留
    tags: etcd,etcd-delete-etcd
    shell: "docker rm -vf etcd"
    ignore_errors: yes
  - name: 拷贝证书文件
    tags: etcd,etcd-copy-certs
    copy:
      src: "/data/etcd/{{ hostname }}/pki/"
      dest: /etc/kubernetes/pki
  - name: 启动etcd的docker容器
    tags: etcd,etcd-run
    shell: "docker run --name etcd --restart always -d -p 2379:2379 -p 2380:2380 \
            -v /var/lib/etcd/:/var/lib/etcd/ \
            -v /etc/kubernetes/:/etc/kubernetes/ \
            -v /etc/localtime:/etc/localtime \
            {{ google_repo }}/etcd:3.4.13-0 etcd \
            --advertise-client-urls=https://{{ inventory_hostname }}:2379 \
            --initial-advertise-peer-urls=https://{{ inventory_hostname }}:2380 \
            --initial-cluster='{{ hostvars[groups.etcd[0]]['hostname'] }}=https://{{ groups.etcd[0] }}:2380,{{ hostvars[groups.etcd[1]]['hostname'] }}=https://{{ groups.etcd[1] }}:2380,{{ hostvars[groups.etcd[2]]['hostname'] }}=https://{{ groups.etcd[2] }}:2380' \
            --initial-cluster-state=new \
            --listen-client-urls=https://0.0.0.0:2379 \
            --listen-peer-urls=https://0.0.0.0:2380 \
            --name={{ hostname }} \
            --client-cert-auth=true \
            --data-dir=/var/lib/etcd \
            --cert-file=/etc/kubernetes/pki/etcd/server.crt \
            --key-file=/etc/kubernetes/pki/etcd/server.key \
            --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt \
            --peer-client-cert-auth=true \
            --peer-key-file=/etc/kubernetes/pki/etcd/peer.key \
            --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \
            --snapshot-count=10000 \
            --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt"
#检查etcd集群状态
#-------------------------------------------------------------------
- hosts: etcd
  tasks:
  - name: 检查etcd集群的状态
    tags: etcd,etcd-check
    shell: "docker run --rm -it \
            --net host \
            -v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.4.13-0 etcdctl \
            --cert /etc/kubernetes/pki/etcd/peer.crt \
            --key /etc/kubernetes/pki/etcd/peer.key \
            --cacert /etc/kubernetes/pki/etcd/ca.crt \
            --endpoints https://{{ inventory_hostname }}:2379 endpoint health --cluster"
    register: checkMsg
- hosts: etcd
  tasks:
  - name: 打印etcd集群状态检查结果，三个节点都为“is healthy”则正常
    tags: etcd,etcd-check,etcd-check-log
    debug: var=checkMsg.stdout
    with_items: checkMsg.results
#初始化第一个master节点
#--------------------------------------------------------------------
- hosts: master[0]
  tasks:
  - name: 拷贝kubeadm-config.yaml配置文件
    tags: master,master-copy-config
    template:
      src: files/kubeadm-config.yaml
      dest: ./kubeadm-config.yaml
  - name: 创建etcd证书目录
    tags: master,master-copy-etcd
    file:
      path: /etc/kubernetes/pki/etcd
      state: directory
  - name: 拷贝etcd证书
    tags: master,master-copy-etcd
    copy:
      src: "/data/etcd/etcd0/{{ item }}"
      dest: "/etc/kubernetes/{{ item }}"
    with_items:
    - pki/etcd/ca.crt
    - pki/apiserver-etcd-client.crt
    - pki/apiserver-etcd-client.key
  - name: 初始化第一个master
    tags: master,master-init
    shell: kubeadm init --config=kubeadm-config.yaml
  - name: 配置kubectl的执行环境
    tags: master,master-env
    shell: "mkdir -p /root/.kube &&\ 
           rm -rf /root/.kube/config &&\ 
           cp /etc/kubernetes/admin.conf /root/.kube/config"
  #安装flannel网络
  - name: 拷贝kube-flannel.yaml文件
    tags: master,flannel
    template:
      src: files/kube-flannel.yaml
      dest: ./kube-flannel.yaml
  - name: 安装flannel网络
    tags: master,flannel
    shell: kubectl apply -f kube-flannel.yaml
  - name: 从master[0]拷贝k8s的config，以便用kubectl操作集群
    tags: master,master-admin
    fetch: src=/etc/kubernetes/admin.conf dest=/root/.kube/config flat=yes
  - name: 从master[0]拷贝证书，其他master节点加入集群的时候要用
    tags: master,master-backup
    fetch: src="/etc/kubernetes/{{ item }}" dest="/data/kubernetes/{{ item }}" flat=yes
    with_items:
    - "pki/ca.crt"
    - "pki/ca.key"
    - "pki/sa.key"
    - "pki/sa.pub"
    - "pki/front-proxy-ca.crt"
    - "pki/front-proxy-ca.key"
    - "pki/apiserver-etcd-client.key"
    - "pki/apiserver-etcd-client.crt"
    - "pki/etcd/ca.crt"
    - "admin.conf"
  - name: 等待第一个master启动
    tags: master,master-wait
    shell: kubectl get pods --all-namespaces | grep -c Running
    register: master_result
    until: master_result.stdout >= "7"
    retries: 9999
    delay: 5
#获得加入集群的token
#--------------------------------------------------------------------
- hosts: master[0]
  tasks:
  - name: 删除所有token
    tags: token
    shell: for token in $(kubeadm token list | tail -n +2 | awk '{print $1}') ; do kubeadm token delete $token; done
  - name: 创建token
    tags: token
    shell: kubeadm token create
  - name: 获得token，赋给master-token变量
    tags: token
    shell: "kubeadm token list | tail -n +2 | head -n 1 | awk '{print $1}'"
    register: master_token
  - name: 获得加入集群的discovery-token-ca-cert-hash并赋值给discovery-token-ca-cert-hash变量
    tags: token
    shell: "openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \
            openssl dgst -sha256 -hex | sed 's/^.* //'"
    register: discovery_token

#其他master加入集群
#--------------------------------------------------------------------
- hosts: master[1],master[2]
  tasks:
  - name: 设置master_token变量
    tags: masters,masters-token
    set_fact: master_token="{{ hostvars[groups.master[0]]['master_token'] }}"
  - name: 设置discovery_token变量
    tags: masters,masters-token
    set_fact: discovery_token="{{ hostvars[groups.master[0]]['discovery_token'] }}"
  - name: 创建证书文件目录
    tags: masters,masters-certs
    file:
      path: /etc/kubernetes/pki/etcd/
      state: directory
  - name: 从控制节点拷贝证书文件
    tags: masters,masters-certs
    copy:
      src: "/data/kubernetes/{{ item }}"
      dest: "/etc/kubernetes/{{ item }}"
    with_items:
    - "pki/ca.crt"
    - "pki/ca.key"
    - "pki/sa.key"
    - "pki/sa.pub"
    - "pki/front-proxy-ca.crt"
    - "pki/front-proxy-ca.key"
    - "pki/apiserver-etcd-client.key"
    - "pki/apiserver-etcd-client.crt"
    - "pki/etcd/ca.crt"
    - "admin.conf"
  - name: 其他master加入集群
    tags: masters,masters-join
    shell: "kubeadm join {{ groups.master[0] }}:6443 \
            --token {{ master_token.stdout }} --discovery-token-ca-cert-hash sha256:{{ discovery_token.stdout }} \
            --control-plane"
- hosts: master[0]
  tasks:
  - name: 等待其他master启动
    tags: masters,masters-wait
    shell: kubectl get pods --all-namespaces | grep -c Running
    register: masters_result
    until: masters_result.stdout >= "17"
    retries: 9999
    delay: 5
  - name: 等待master启动，1分钟
    tags: masters,masters-wait
    shell: sleep 60
#node加入集群
#--------------------------------------------------------------------
- hosts: node
  tasks:
  - name: 设置master_token变量
    tags: nodes,node-token
    set_fact: master_token="{{ hostvars[groups.master[0]]['master_token'] }}"
  - name: 设置discovery_token变量
    tags: nodes,node-token
    set_fact: discovery_token="{{ hostvars[groups.master[0]]['discovery_token'] }}"
  - name: node加入集群
    tags: nodes,node-join
    shell: "kubeadm join {{ loadbalance_inside_ip }}:6443 \
            --token {{ master_token.stdout }} --discovery-token-ca-cert-hash sha256:{{ discovery_token.stdout }}"
  - name: 等待node启动，1分钟
    tags: nodes,node-wait
    shell: sleep 60
